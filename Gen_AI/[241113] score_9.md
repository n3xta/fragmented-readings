前言：好久没有玩生成式ai了，因为备战ib大考导致我和这个社区有整整半年多的脱节，有海量的新知识和新工具要学；但是舒适圈肯定是要踏出的，今天终于想起要把ComfyUI这个人手一份的工作流搞定。
秋叶大佬的整合包是没有模型的，正好我webui时期用的模型也都太老了，就去civitai上逛逛大家最近都在用什么，顺便学学prompt。

涩图就不贴在这里了，[大家想看自己点开](https://civitai.com/images/39864984)，主要是这张效果真的很不错，构图、线条、人物表情都很过得去。但是

<img src="https://raw.githubusercontent.com/n3xta/image-hosting/main/img/202411131854773.png"/>

这一串score_9,score_8_up,score_7_up,score_6_up看的我云里雾里，而且还用括号做了强调。negative prompt里也有出现。

顺藤摸瓜，找到了这个文章：[What is score_9 and how to use it in Pony Diffusion](https://civitai.com/articles/4248/what-is-score9-and-how-to-use-it-in-pony-diffusion)，下面是笔记

# 背景知识

模型的生命周期主要分为两个阶段：训练和推理。这个标签就是在推理阶段产生的。

模型本身并不能理解“好看”或“高质量”的概念，因此生成图像的质量很难保证。而且，ai生成的图像质量通常取决于训练时使用的数据（即“垃圾进，垃圾出”原则）。虽然可以选择只用高质量数据来训练模型，但很多概念并没有足够的优质数据。

## CLIP

为了让模型生成好看的图像，大家采用一种叫做**Contrastive Language-Image Pre-training**的方法，核心就是**利用自然语言的监督信号来训练一个比较好的视觉模型**。

### 插播一段关于CLIP的笔记

**Paper：Learning Transferable Visual Models From Natural Language Supervision**

<img src="https://raw.githubusercontent.com/n3xta/image-hosting/main/img/202411131918386.png"/>

[github repo](https://github.com/OpenAI/CLIP)
[这篇博客讲得很清楚](https://blog.csdn.net/h661975/article/details/135116957)

就不深入研究了，量子速读一下，它做了一个zero shot的迁移学习，好处是：
1. 如果只需要下载*图片-文本*，别的标注都不需要做，那么数据的规模很容易就变大。（像ImageNet需要先定好1000个类，而这1000个类也需要之前筛选选好，根据这些类去*下载图片→清理数据集→标注*，这个过程比较复杂。）
2. 接上文，现在的监督信号是一个文本，而不是N选1的标签，模型的输入输出自由度就大了很多。
3. 因为训练时把图片和文本绑定到一起，那么训练的特征就不再仅是一个视觉特征了，而是一个多模态的特征，也就很容易去做 zero-shot 的迁移学习。（如果只是做单模态的自监督学习，无论是单模态的对比学习（如MOCO），还是单模态的掩码学习（如MAE），都只能学到视觉特征，而无法与自然语言联系到一起，这样还是很难做 zero-shot 迁移。）

### 继续

总之就是，模型依靠人类图像描述中的关键词（如“masterpiece”“best quality”等）理解“美学”概念。那么为了让模型知道什么时候是好看的，就可以利用自然语言进行训练，即“标注质量分”。

## 标签打分的使用

为了达到这个效果，作者（？）找了很多质量不一的图片，手动为每张图片评分。高质量的图像会被标记为 score_9，稍差的标记为 score_8，以此类推。训练过程中，模型会学习这些标签，了解哪类标签对应的图像质量更高。经过数周的数据标注后，生成了一个足够大的美学数据集。

## 总结

在训练Pony Diffusion的时候，他们手动加入了一个质量提示词，以便在生成图像时更好地控制质量。

搞明白之后我都笑嘻了，让我想起隔壁midjourney用那个什么IMG_1145来做提示词。